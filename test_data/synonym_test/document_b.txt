The most famous type of preservation problem is “link rot”, a situation where a hyperlink no longer functions because the destination material has vanished. A related phenomenon to this is where the content of a linked page alters such that it no longer reflects the state at the point that a link was made. This is known as “content drift”, a phrase initially set out by Jones et al. In 2016, and to which much of this article is indebted.1
Content drift is a hard term to define, as there are different types of content on web pages, some of which are important and others of which are not. For example, if there is an ad on a page that changes, but the main content body stays the same, has the content changed? Likewise when a major website redesigns their site, but keeps the central content, is this content drift? The difference lies between the bitstream that is sent to the client changing vs. the human-readable content changing. Even this binary, though, is not that obvious: if a small typo is corrected, is this content drift? Thus, the decision about whether something is or is not content drifted is subjective.
This definitional challenge recurs in existing articles on the topic. For instance, Cho and Garcia-Molina report that “by change we mean any change to the textual content of a page”, using the checksum of the page’s HTML as the marker of whether the page has changed.2 Cho and Garcia-Molina’s primary finding was that “more than 40% of pages in the com domain changed every day, while less than 10% of the pages in other domains changed at that frequency” and the ambition of their paper was to construct a mathematical model for change periodicity, allowing a web crawler to avoid batch refreshing its entire database on every crawl.3
Problematically, when adopting the byte-wise comparison methods, some (much older) studies have shown that 16.5% of sites altered on every visit, even if the two visits were immediately one after another.4 In order to avoid such problems, not all approaches to detecting content drift have been automated. Bowers et al. surveyed 4,500 papers in a “human-reviewed” sample, implying that their definition of drift cannot be the precision of the digital bytestream, but rather was looking for “the information journalists had intended”.