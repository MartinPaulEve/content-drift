The most well-known form of preservation challenge is “link rot”, a situation where a hyperlink no longer functions because the destination material has disappeared. A related phenomenon to this is where the content of a hyperlinked page changes such that it no longer reflects the state at the time that a link was made. This is known as “content drift”, a term initially set out by Jones et al. In 2016, and to which much of this work is indebted.1
Content drift is a difficult term to define, as there are different categories of content on web pages, some of which matter and others of which do not. For instance, if there is an advert on a web page that changes, but the main content body remains the same, has the content drifted? Likewise when a major platform redesigns their site, but retains the central content, is this content drift? The difference lies between the bitstream that is delivered to the client varying vs. the human-readable content changing. Even this binary, though, is not that clear: if a small typo is corrected, does this constitute content drift? Thus, the decision about whether something is or is not content drifted is subjective.
This definitional problem recurs in existing papers on the topic. For instance, Cho and Garcia-Molina report that “by change we mean any change to the textual content of a page”, using the checksum of the page’s HTML as the marker of whether the page has changed.2 Cho and Garcia-Molina’s primary finding was that “more than 40% of pages in the com domain changed every day, while less than 10% of the pages in other domains changed at that frequency” and the goal of their paper was to construct a mathematical model for change periodicity, allowing a web crawler to avoid batch refreshing its entire database on every crawl.3
Problematically, when adopting the byte-wise comparison methods, some (much older) studies have shown that 16.5% of websites changed on every visit, even if the two visits were immediately consecutive.4 In order to avoid such problems, not all approaches to detecting content drift have been automated. Bowers et al. surveyed 4,500 papers in a “human-reviewed” sample, implying that their definition of drift cannot be the precision of the digital bytestream, but instead was looking for “the information journalists had intended”.